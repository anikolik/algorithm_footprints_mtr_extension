{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwQUy_n2A8_X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /#YOUR PATH TO THE NOTEBOOK FOLDER IN GOOGLE COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_uV3xTsMDhc"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4CD1AAVGoC_"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import itertools\n",
        "import shap\n",
        "import tensorflow as tf\n",
        "# Set seed for Python's random module\n",
        "random.seed(42)\n",
        "# Set seed for NumPy's random generator\n",
        "np.random.seed(42)\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from configuration import data_path as DATA_PATH\n",
        "from configuration import results_root as RESULTS_ROOT\n",
        "from configuration import results_path as RESULTS_PATH\n",
        "from configuration import model_name as MODEL_NAME\n",
        "from configuration import hyperparameter_tuning as HYPERPARAMETER_TUNNING\n",
        "from configuration import feature_selection as FEATURE_SELECTION\n",
        "\n",
        "from configuration import batchsize as BATCH_SIZE\n",
        "from configuration import epochs as EPOCHS"
      ],
      "metadata": {
        "id": "n4M2zTKTNZ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import load_data, load_selected_features, load_hyperparameters, init_model, dimensionality_reduction_2D"
      ],
      "metadata": {
        "id": "9BuVr-0tzzc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9P-VE2OMUoN"
      },
      "outputs": [],
      "source": [
        "DATA_PATH, RESULTS_ROOT, MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsM72sv2OIxU"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4mxYX6u6KP4"
      },
      "source": [
        "FOOTPRINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEm9Y6jn-ZGD"
      },
      "outputs": [],
      "source": [
        "class Footprint:\n",
        "  def __init__(self, model_name, hyperparameter_tuning, feature_selection):\n",
        "    self.model_name = model_name\n",
        "    self.hyperparameter_tuning = hyperparameter_tuning\n",
        "    self.feature_selection = feature_selection\n",
        "    self.model = None\n",
        "    self.data = {}\n",
        "    self.shapley_values = None\n",
        "    self.shapley_feature_importance = None\n",
        "    self.meta_representations = None\n",
        "    self.meta_representations_2d = None\n",
        "    self.dim_reduction = None\n",
        "\n",
        "  def train_model(self, data_path, results_path, **kwargs):\n",
        "    # Load data\n",
        "    X_train, y_train, X_test, y_test = load_data(path=data_path)\n",
        "\n",
        "    if self.feature_selection:\n",
        "      selected_features = load_selected_features(path=f\"{results_path}/{self.model_name}/hyperparameter_tuning=True-feature_selection=True\", n=None)\n",
        "      print(f\"\\nLoading selected features: {selected_features}\")\n",
        "\n",
        "      X_train = X_train[selected_features]\n",
        "      X_test = X_test[selected_features]\n",
        "\n",
        "    # Load previously tuned hyperparameters or set default parameters\n",
        "    params = load_hyperparameters(model_name=self.model_name, results_root=results_path, hyperparameter_tuning=self.hyperparameter_tuning)\n",
        "    print(f\"\\nLoading hyperparameters: {params}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = init_model(params=params, model_name=self.model_name, num_features=X_train.shape[1], num_targets=y_train.shape[1])\n",
        "\n",
        "    if self.model_name == \"neural_network\":\n",
        "      model.fit(X_train, y_train, batch_size=kwargs['batch_size'], epochs=kwargs['epochs'], verbose=1)\n",
        "    else:\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "    self.model = model\n",
        "    self.data['X_train'] = X_train\n",
        "    self.data['X_test'] = X_test\n",
        "    self.data['y_train'] = y_train\n",
        "    self.data['y_test'] = y_test\n",
        "    return self\n",
        "\n",
        "  def calculate_shapley_values(self):\n",
        "    \"\"\" Calculate Shapley values for multi target regression model, with respect to each target. \"\"\"\n",
        "    explainer = shap.KernelExplainer(model=self.model.predict, data=self.data[\"X_test\"].values)\n",
        "    shapley_values_mtr = explainer.shap_values(X=self.data[\"X_test\"].values)\n",
        "    shapley_values_mtr = [shapley_values_mtr[:,:,i] for i in range(len(self.data[\"y_test\"].columns))]\n",
        "    for target_id, shapley_values in zip(self.data[\"y_test\"].columns, shapley_values_mtr):\n",
        "      self.shapley_values = {target_id: pd.DataFrame(shapley_values, columns=self.data[\"X_test\"].columns, index=self.data[\"X_test\"].index) for target_id, shapley_values in zip(self.data[\"y_test\"].columns, shapley_values_mtr)}\n",
        "      expected_values_mtr = explainer.expected_value\n",
        "      self.expected_values = {target_id: expected_value for target_id, expected_value in zip(self.data[\"y_test\"].columns, expected_values_mtr)}\n",
        "      self.explainer = explainer\n",
        "    return self\n",
        "\n",
        "  def calculate_shapley_feature_importance(self):\n",
        "    \"\"\" Calculate Shapley importance by averaging the Shapley values per feature. \"\"\"\n",
        "    shapley_feature_importance = {}\n",
        "    for target_id, shapley_values in self.shapley_values.items():\n",
        "      shapley_importance_temp = shapley_values.abs().mean().reset_index()\n",
        "      shapley_importance_temp = shapley_importance_temp.rename(columns={\"index\": \"feature_name\", 0: 'feature_importance'}).sort_values(by='feature_importance', ascending=False)\n",
        "      shapley_importance_temp['rank'] = np.arange(1, len(shapley_importance_temp) + 1)\n",
        "      shapley_feature_importance[target_id] = shapley_importance_temp\n",
        "    self.shapley_feature_importance = shapley_feature_importance\n",
        "    return self\n",
        "\n",
        "  def concat_metarepresentations(self):\n",
        "    \"\"\" Concatenate shapley values for all algorithms into a single DataFrame. \"\"\"\n",
        "    data = pd.DataFrame()\n",
        "    for target_id, shap_values in self.shapley_values.items():\n",
        "      print(f\"\\ntarget_id: {target_id}\")\n",
        "      data = pd.concat([data, shap_values.assign(target_id=target_id)], axis=0)\n",
        "    data = data.set_index(\"target_id\", append=True)\n",
        "    self.meta_representations = data\n",
        "    return data\n",
        "\n",
        "  def metarepresentations_2d(self):\n",
        "    \"\"\" Dimensionality reduction 2D. \"\"\"\n",
        "    data_2d, dim_reduction = dimensionality_reduction_2D(data=self.meta_representations, dim_reduction_name=\"PCA\", n_components=2)\n",
        "    self.meta_representations_2d = data_2d\n",
        "    self.dim_reduction = dim_reduction\n",
        "    return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75BCkGTOL2UI"
      },
      "source": [
        "RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-BKiykRGET3"
      },
      "outputs": [],
      "source": [
        "def run_footprint():\n",
        "  my_file = Path(f\"{RESULTS_PATH}/footprint.joblib\")\n",
        "  print(my_file.is_file())\n",
        "  if my_file.is_file():\n",
        "      footprint = joblib.load(f\"{RESULTS_PATH}/footprint.joblib\")\n",
        "      print(f\"\\n\\n--- Load data ---\")\n",
        "      for dataset in [\"train\", \"test\"]:\n",
        "        print(f\"{dataset}: \\n\")\n",
        "        print(footprint.data[f\"X_{dataset}\"].head())\n",
        "        print(footprint.data[f\"X_{dataset}\"].shape)\n",
        "        print(\"\\n\")\n",
        "        print(footprint.data[f\"y_{dataset}\"].head())\n",
        "        print(footprint.data[f\"y_{dataset}\"].shape)\n",
        "  else:\n",
        "    footprint = Footprint(model_name=MODEL_NAME, hyperparameter_tuning=HYPERPARAMETER_TUNNING, feature_selection=FEATURE_SELECTION)\n",
        "\n",
        "    footprint.train_model(data_path=DATA_PATH, results_path=RESULTS_ROOT)\n",
        "    print(f\"\\n\\n--- Load data ---\")\n",
        "    for dataset in [\"train\", \"test\"]:\n",
        "      print(f\"{dataset}: \\n\")\n",
        "      print(footprint.data[f\"X_{dataset}\"].head())\n",
        "      print(footprint.data[f\"X_{dataset}\"].shape)\n",
        "      print(\"\\n\")\n",
        "      print(footprint.data[f\"y_{dataset}\"].head())\n",
        "      print(footprint.data[f\"y_{dataset}\"].shape)\n",
        "\n",
        "    footprint.calculate_shapley_values()\n",
        "    print(f\"\\n\\n--- Calculate Shapley values ---\")\n",
        "    for target_id, shapley_values in footprint.shapley_values.items():\n",
        "      print(f\"\\n{target_id}\")\n",
        "      print(shapley_values.head())\n",
        "      print(shapley_values.shape)\n",
        "\n",
        "    # save fitted model\n",
        "    joblib.dump(footprint, f\"{RESULTS_PATH}/footprint.joblib\")\n",
        "\n",
        "  footprint.calculate_shapley_feature_importance()\n",
        "  print(f\"--- Calculate Shapley importance --- \\n\")\n",
        "  for target_id, shapley_feature_importance in footprint.shapley_feature_importance.items():\n",
        "    print(f\"{target_id} \\n\")\n",
        "    print(shapley_feature_importance.head(30))\n",
        "    print(shapley_feature_importance.shape)\n",
        "\n",
        "  footprint.concat_metarepresentations()\n",
        "  print(f\"--- Concatenate meta-representations --- \\n\")\n",
        "  print(footprint.meta_representations.head())\n",
        "  print(footprint.meta_representations.shape)\n",
        "\n",
        "  footprint.metarepresentations_2d()\n",
        "  print(f\"--- Meta-representations 2D --- \\n\")\n",
        "  print(footprint.meta_representations_2d.head())\n",
        "  print(footprint.meta_representations_2d.shape)\n",
        "\n",
        "  # save to csv\n",
        "  for target_id, shapley_values in footprint.shapley_values.items():\n",
        "    shapley_values.reset_index().to_csv(f\"{RESULTS_PATH}/shapley_values-{target_id}.csv\")\n",
        "\n",
        "  # save fitted model\n",
        "  joblib.dump(footprint, f\"{RESULTS_PATH}/footprint.joblib\")\n",
        "\n",
        "run_footprint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}