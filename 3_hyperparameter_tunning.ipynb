{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab_Notebooks/footprints_mtr"
      ],
      "metadata": {
        "id": "lwQUy_n2A8_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "sfjNNUCDG619"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour, plot_parallel_coordinate\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from sklearn.linear_model import MultiTaskElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Set seed for Python's random module\n",
        "random.seed(42)\n",
        "# Set seed for NumPy's random generator\n",
        "np.random.seed(42)\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "M4CD1AAVGoC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from configuration import data_path as DATA_PATH\n",
        "from configuration import results_path as RESULTS_PATH\n",
        "from configuration import model_name as MODEL_NAME\n",
        "from configuration import n_splits as N_SPLITS\n",
        "from configuration import n_trials as N_TRIALS\n",
        "from configuration import batchsize as BATCH_SIZE\n",
        "from configuration import epochs as EPOCHS"
      ],
      "metadata": {
        "id": "JGgRX6duKfrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME, N_SPLITS, N_TRIALS, BATCH_SIZE, EPOCHS"
      ],
      "metadata": {
        "id": "Zg_tcK94Cfl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH, RESULTS_PATH"
      ],
      "metadata": {
        "id": "6SJWWM4daEZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create stratified train-validation splits\n",
        "def train_validation_split(n_splits=5):\n",
        "    \"\"\"Create cross-validation splits.\"\"\"\n",
        "    X_train = pd.read_csv(f\"{DATA_PATH}/X_train.csv\", index_col=[\"f_id\", \"i_id\"])\n",
        "    y_train = pd.read_csv(f\"{DATA_PATH}/y_train.csv\", index_col=[\"f_id\", \"i_id\"])\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    splits = [\n",
        "        (X_train.iloc[train_idx], X_train.iloc[valid_idx], y_train.iloc[train_idx], y_train.iloc[valid_idx])\n",
        "        for train_idx, valid_idx in skf.split(X_train, X_train.index.get_level_values(\"f_id\"))\n",
        "    ]\n",
        "    return splits\n",
        "\n",
        "# Function to create different models based on the trial's hyperparameters\n",
        "def create_model(trial, num_features: int, num_targets: int):\n",
        "    \"\"\"Create and return a model based on the model name and trial hyperparameters.\"\"\"\n",
        "\n",
        "    if MODEL_NAME == \"multitask_elastic_net\":\n",
        "        alpha = trial.suggest_loguniform(\"alpha\", 1e-5, 1e5)\n",
        "        l1_ratio = trial.suggest_uniform(\"l1_ratio\", 0, 1)\n",
        "        model = MultiTaskElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "\n",
        "    elif MODEL_NAME == \"random_forest\":\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
        "        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None, 1.0])\n",
        "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators, max_depth=max_depth, max_features=max_features,\n",
        "            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=42\n",
        "        )\n",
        "\n",
        "    elif MODEL_NAME == \"neural_network\":\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(num_features,)))\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            num_hidden = trial.suggest_int(f\"n_units_l{i}\", 4, 128, log=True)\n",
        "            model.add(Dense(num_hidden, activation=\"relu\", kernel_initializer=\"normal\"))\n",
        "            dropout = trial.suggest_float(f\"dropout_l{i}\", 0.2, 0.5)\n",
        "            model.add(Dropout(rate=dropout, seed=seed_value))\n",
        "\n",
        "        model.add(Dense(num_targets))\n",
        "        model.compile(loss=\"mean_absolute_error\", optimizer=\"adam\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model name: {MODEL_NAME}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Objective function for Optuna hyperparameter optimization\n",
        "def objective(trial):\n",
        "    splits = train_validation_split()\n",
        "    scores = []\n",
        "\n",
        "    for X_train, X_valid, y_train, y_valid in splits:\n",
        "        model = create_model(trial, num_features=X_train.shape[1], num_targets=y_train.shape[1])\n",
        "\n",
        "        if MODEL_NAME == \"neural_network\":\n",
        "            model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=False)\n",
        "            score = mean_absolute_error(y_valid, model.predict(X_valid))\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            score = mean_absolute_error(y_valid, model.predict(X_valid))\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "\n",
        "# Function to run hyperparameter optimization\n",
        "def run_hyperparameter_optimization():\n",
        "    \"\"\"Run Optuna hyperparameter optimization.\"\"\"\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(n_startup_trials=40, seed=42))\n",
        "\n",
        "    # Enqueue some manually tested values\n",
        "    if MODEL_NAME == \"multitask_elastic_net\":\n",
        "        trials = [\n",
        "            {\"alpha\": 1.0, \"l1_ratio\": 0.5},\n",
        "            {\"alpha\": 0.5, \"l1_ratio\": 1},\n",
        "            {\"alpha\": 1e-8, \"l1_ratio\": 0.0},\n",
        "            {\"alpha\": 1.0, \"l1_ratio\": 0.0},\n",
        "            {\"alpha\": 1e-7, \"l1_ratio\": 1.0},\n",
        "            {\"alpha\": 1.0, \"l1_ratio\": 1.0},\n",
        "            {\"alpha\": 1e-7, \"l1_ratio\": 0.5},\n",
        "            {\"alpha\": 0.5, \"l1_ratio\": 0.0}\n",
        "        ]\n",
        "        for t in trials:\n",
        "            study.enqueue_trial(t)\n",
        "\n",
        "    elif MODEL_NAME == \"random_forest\":\n",
        "        study.enqueue_trial({\n",
        "            'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2,\n",
        "            'max_features': 1.0, 'min_impurity_decrease': 0.0, 'ccp_alpha': 0.0, 'random_state': 42\n",
        "        })\n",
        "\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    print(f\"Best trial number: {study.best_trial.number}\")\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "    print(f\"Best score: {study.best_value}\")\n",
        "\n",
        "    # Save the best parameters and study results\n",
        "    joblib.dump(study.best_params, f\"{RESULTS_PATH}/best_params.joblib\")\n",
        "    joblib.dump(study, f\"{RESULTS_PATH}/optuna_study.pkl\")\n",
        "    study.trials_dataframe().to_csv(f\"{RESULTS_PATH}/optimization_report.csv\")\n",
        "\n",
        "    # Visualize the hyperparameter search process\n",
        "    visualize_hyperparameter_search(study, save_path=RESULTS_PATH)\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "\n",
        "# Function to visualize Optuna hyperparameter optimization results\n",
        "def visualize_hyperparameter_search(study, save_path):\n",
        "    \"\"\"Visualize the Optuna hyperparameter search process.\"\"\"\n",
        "    plots = [\n",
        "        (plot_optimization_history, \"optimization_history\"),\n",
        "        (plot_param_importances, \"param_importances\"),\n",
        "        (plot_contour, \"contour_plot\"),\n",
        "        (plot_parallel_coordinate, \"parallel_coordinate\")\n",
        "    ]\n",
        "\n",
        "    for plot_fn, filename in plots:\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plot_fn(study)\n",
        "        plt.savefig(f\"{save_path}/{filename}.png\", bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "DiJ7dpQnGXPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN HYPERPARAMETER OPTIMIZATION"
      ],
      "metadata": {
        "id": "T0-Y17KFQ1mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = run_hyperparameter_optimization()"
      ],
      "metadata": {
        "id": "RtJbBxdJGfdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "Gm6Q2_80RsQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a sequential neural network model\n",
        "def create_sequential_model(params, num_features, num_targets):\n",
        "    \"\"\"\n",
        "    Create a neural network model with layers based on the parameters provided by Optuna.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(num_features,)))  # Input layer\n",
        "    # Add hidden layers dynamically based on n_layers parameter\n",
        "    for i in range(params['n_layers']):\n",
        "        model.add(Dense(params[f'n_units_l{i}'], activation=\"relu\", kernel_initializer=\"normal\"))\n",
        "        model.add(Dropout(rate=params[f'dropout_l{i}']))\n",
        "    model.add(Dense(num_targets))  # Output layer with num_targets\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss=\"mean_absolute_error\",\n",
        "        optimizer=\"adam\"\n",
        "    )\n",
        "    model.summary()  # Display the model summary\n",
        "    return model\n",
        "\n",
        "def plot_learning_curve(params, save_path):\n",
        "    \"\"\" Train the model and plot the learning curve for each cross-validation split. \"\"\"\n",
        "    splits = train_validation_split()\n",
        "    i = 0\n",
        "    for X_train, X_valid, y_train, y_valid in splits:\n",
        "      # Create the model with the current parameters\n",
        "      model = create_sequential_model(params=params, num_features=X_train.shape[1], num_targets=y_train.shape[1])\n",
        "\n",
        "      # Fit the model and save the training history\n",
        "      history = model.fit(X_train, y_train, epochs=100, batch_size=BATCH_SIZE, validation_data=(X_valid, y_valid), verbose=0)\n",
        "\n",
        "      # Evaluate model on the validation set\n",
        "      model.evaluate(X_valid, y_valid, verbose=1)\n",
        "\n",
        "      # Plot training and validation loss\n",
        "      plt.plot(history.history['loss'], label='Train Loss')\n",
        "      plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "      plt.title(f'Model Loss - Split {i}')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.legend(loc='upper right')\n",
        "      i = i + 1\n",
        "      # Save the plot\n",
        "      plt.savefig(f'{save_path}/learning_curve_{i}.png', bbox_inches='tight', dpi=300)\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "jccogZekUrZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOT LEARNING CURVE"
      ],
      "metadata": {
        "id": "ZtvyPykpRx6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_NAME == \"neural_network\":\n",
        "  plot_learning_curve(params=params, save_path=RESULTS_PATH)"
      ],
      "metadata": {
        "id": "Nu97oxcicgrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsBOEzxuYq7z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}