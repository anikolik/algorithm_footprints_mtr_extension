{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# %cd #YOUR PATH TO THE NOTEBOOK FOLDER IN GOOGLE COLAB"
      ],
      "metadata": {
        "id": "ZOJaXbtR0MHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import joblib\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import MultiTaskElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "\n",
        "# Set seed for Python's random module\n",
        "random.seed(42)\n",
        "# Set seed for NumPy's random generator\n",
        "np.random.seed(42)\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "Jpjl9Boy0NSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from configuration import data_path as DATA_PATH\n",
        "from configuration import results_root as RESULTS_ROOT\n",
        "from configuration import model_eval_path as MODEL_EVAL_PATH\n",
        "from configuration import n_splits as N_SPLITS\n",
        "from configuration import batchsize as BATCH_SIZE\n",
        "from configuration import epochs as EPOCHS"
      ],
      "metadata": {
        "id": "Hmsn4qO7pofE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "metadata": {
        "id": "0Wtq5OLw_KXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH, RESULTS_ROOT, MODEL_EVAL_PATH"
      ],
      "metadata": {
        "id": "wjgI6rExqaGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path: str):\n",
        "    \"\"\"\n",
        "    Load training data from CSV files.\n",
        "    Args:\n",
        "        path (str): The directory path where the training data CSV files are stored.\n",
        "    Returns:\n",
        "        X_train (pd.DataFrame): A DataFrame containing the input features for training.\n",
        "        y_train (pd.DataFrame): A DataFrame containing the target labels for training.\n",
        "    \"\"\"\n",
        "    X_train = pd.read_csv(f\"{path}/X_train.csv\", index_col=[\"f_id\", \"i_id\"])\n",
        "    y_train = pd.read_csv(f\"{path}/y_train.csv\", index_col=[\"f_id\", \"i_id\"])\n",
        "\n",
        "    X_test = pd.read_csv(f\"{path}/X_test.csv\", index_col=[\"f_id\", \"i_id\"])\n",
        "    y_test = pd.read_csv(f\"{path}/y_test.csv\", index_col=[\"f_id\", \"i_id\"])\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "_Hf0zJkplLzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hyperparameters(model_name: str, results_root: str, hyperparameter_tuning: bool) -> dict:\n",
        "    \"\"\"\n",
        "    Load previously tuned hyperparameters or set default parameters.\n",
        "    Args:\n",
        "        model_name (str): The name of the model (e.g., 'random_forest', 'multitask_elastic_net').\n",
        "        results_root (str): The root directory where results are saved.\n",
        "        hyperparameter_tuning (bool): If true, load the tuned hyperparameters from file.\n",
        "    Returns:\n",
        "        dict: The hyperparameters for the model.\n",
        "    \"\"\"\n",
        "    if hyperparameter_tuning:\n",
        "        params = joblib.load(f\"{results_root}/{model_name}/hyperparameter_tuning=True-feature_selection=False/best_params.joblib\")\n",
        "    else:\n",
        "        if model_name == \"random_forest\":\n",
        "            params = {\"random_state\": 42}\n",
        "        elif model_name == \"multitask_elastic_net\":\n",
        "            params = {}\n",
        "        elif model_name == \"neural_network\":\n",
        "            params = {'n_layers': 1,\n",
        "                      'n_units_l0': 32,\n",
        "                      'dropout_l0': 0.2,\n",
        "                      }\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model name\")\n",
        "\n",
        "    return params\n",
        "\n",
        "def init_model(params: dict, model_name: str, num_features, num_targets):\n",
        "    \"\"\"\n",
        "    Initialize a machine learning model based on the model name and provided parameters.\n",
        "    Args:\n",
        "        params (dict): A dictionary containing the hyperparameters for the model.\n",
        "        model_name (str): A string specifying the name of the model to be initialized.\n",
        "                          Supported values are \"multitask_elastic_net\" and \"random_forest\".\n",
        "    Returns:\n",
        "        model: The initialized machine learning model, either MultiTaskElasticNet or RandomForestRegressor.\n",
        "    \"\"\"\n",
        "    if model_name == \"multitask_elastic_net\":\n",
        "        return MultiTaskElasticNet(**params)\n",
        "    elif model_name == \"random_forest\":\n",
        "        # Ensure random_state is set for RandomForest if not provided\n",
        "        if \"random_state\" not in params:\n",
        "            params[\"random_state\"] = 42\n",
        "        return RandomForestRegressor(**params)\n",
        "    elif model_name == \"neural_network\":\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(num_features,)))  # Input layer\n",
        "        # Add hidden layers dynamically based on n_layers parameter\n",
        "        for i in range(params['n_layers']):\n",
        "            model.add(Dense(params[f'n_units_l{i}'], activation=\"relu\", kernel_initializer=\"normal\"))\n",
        "            model.add(Dropout(rate=params[f'dropout_l{i}'], seed=42))\n",
        "        model.add(Dense(num_targets))  # Output layer with num_targets\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            loss=\"mean_absolute_error\",\n",
        "            optimizer=\"adam\"\n",
        "        )\n",
        "        model.summary()  # Display the model summary\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model name\")"
      ],
      "metadata": {
        "id": "a2MjR7bsViyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test, n_splits=None, cv=None):\n",
        "  \"\"\"\n",
        "  Evaluate a regression model using a predefined cross-validation strategy and on the test set with multiple metrics.\n",
        "  Returns cross-validation scores and test scores for MAE, MSE, and R².\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  model : estimator object\n",
        "      The regression model to evaluate.\n",
        "\n",
        "  X_train : pandas.DataFrame or numpy.ndarray\n",
        "      Training data features.\n",
        "\n",
        "  y_train : pandas.Series or numpy.ndarray\n",
        "      Training data target.\n",
        "\n",
        "  X_test : pandas.DataFrame or numpy.ndarray\n",
        "      Test data features.\n",
        "\n",
        "  y_test : pandas.Series or numpy.ndarray\n",
        "      Test data target.\n",
        "\n",
        "  cv : cross-validation strategy or int, optional\n",
        "      A predefined cross-validation splitting strategy (e.g., KFold object). If None, defaults to 5-fold cross-validation.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  cv_scores : dict\n",
        "      A dictionary containing the mean cross-validation scores for MAE, MSE, and R².\n",
        "\n",
        "  test_scores : dict\n",
        "      A dictionary containing the test set scores for MAE, MSE, and R².\n",
        "  \"\"\"\n",
        "  # Use provided cross-validation strategy or default to KFold with 5 splits\n",
        "  if cv is None:\n",
        "      cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "  # Lists to store scores for each target\n",
        "  targets = y_train.columns\n",
        "  r2_scores = []\n",
        "  mae_scores = []\n",
        "  cv_scores = []\n",
        "  test_scores = []\n",
        "\n",
        "  # Cross-validation loop\n",
        "  for fold, (train_index, test_index) in enumerate(cv.split(X_train, X_train.index.get_level_values(\"f_id\"))):\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    if model_name == \"neural_network\":\n",
        "      model.fit(X_train_fold, y_train_fold, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=False)\n",
        "    else:\n",
        "      model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Make predictions on the test fold\n",
        "    y_pred_fold = model.predict(X_val_fold)\n",
        "\n",
        "    # Calculate R^2 and MAE for each target and append the results\n",
        "    fold_r2_scores = r2_score(y_val_fold, y_pred_fold, multioutput='raw_values')\n",
        "    fold_mae_scores = mean_absolute_error(y_val_fold, y_pred_fold, multioutput='raw_values')\n",
        "\n",
        "    # Organize results into a DataFrame\n",
        "    for i, target in enumerate(targets):\n",
        "            # Append R^2 score\n",
        "            cv_scores.append({\n",
        "                'Fold': fold + 1,\n",
        "                'Target': target,\n",
        "                'Scorer': 'R2',\n",
        "                'Score': fold_r2_scores[i]\n",
        "            })\n",
        "            # Append MAE score\n",
        "            cv_scores.append({\n",
        "                'Fold': fold + 1,\n",
        "                'Target': target,\n",
        "                'Scorer': 'MAE',\n",
        "                'Score': fold_mae_scores[i]\n",
        "            })\n",
        "  # Convert results list to a DataFrame\n",
        "  cv_scores = pd.DataFrame(cv_scores)\n",
        "\n",
        "  # Group by 'Target' and 'Scorer' to calculate the mean score for each combination\n",
        "  cv_scores = cv_scores.groupby(['Target', 'Scorer'])['Score'].mean().reset_index()\n",
        "\n",
        "  cv_scores = cv_scores.sort_values(\n",
        "  by=['Scorer', 'Target'],\n",
        "  ascending=[True, True],\n",
        "  )   # Sort by Scorer ('MAE' ascending, 'R2' descending), then by Score and Target\n",
        "\n",
        "  #################################################################################\n",
        "  # Fit the model on the full training set\n",
        "  if model_name == \"neural_network\":\n",
        "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=False)\n",
        "  else:\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  # Predict on the test set\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Calculate R^2 and MAE for each target\n",
        "  r2_scores = r2_score(y_test, y_pred, multioutput='raw_values')\n",
        "  mae_scores = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
        "\n",
        "  for i, target in enumerate(targets):\n",
        "    # Append R^2 score\n",
        "    test_scores.append({\n",
        "        'Target': target,\n",
        "        'Scorer': 'R2',\n",
        "        'Score': r2_scores[i]\n",
        "    })\n",
        "    # Append MAE score\n",
        "    test_scores.append({\n",
        "        'Target': target,\n",
        "        'Scorer': 'MAE',\n",
        "        'Score': mae_scores[i]\n",
        "    })\n",
        "\n",
        "  # Convert results list to a DataFrame\n",
        "  test_scores = pd.DataFrame(test_scores)\n",
        "\n",
        "  test_scores = test_scores.sort_values(\n",
        "  by=['Scorer', 'Target'],\n",
        "  ascending=[True, True],\n",
        "  )   # Sort by Scorer ('MAE' ascending, 'R2' descending), then by Score and Target\n",
        "\n",
        "  return cv_scores, test_scores"
      ],
      "metadata": {
        "id": "Yl4hrxQd5o10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_selected_features(path, n=None):\n",
        "    \"\"\"\n",
        "    Load selected features based on a feature selection report.\n",
        "\n",
        "    This function loads a CSV file that contains the results of a feature selection process.\n",
        "    The user can either retrieve the features associated with the minimum average score (`avg_score`),\n",
        "    or, if `n` is specified, retrieve features corresponding to a specific number of features (or another\n",
        "    criterion defined by 'n').\n",
        "\n",
        "    Args:\n",
        "        path (str): The path to the feature selection report CSV file.\n",
        "        n (int, optional): The number of features (or another selection criterion) to filter the report.\n",
        "                           If not specified, the features with the minimum `avg_score` will be returned.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of selected feature names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the feature selection report\n",
        "    selected_features_df = pd.read_csv(f\"{path}/feature_selection_report.csv\")\n",
        "\n",
        "    # If n is not specified, use the features associated with the minimum avg_score\n",
        "    if n is None:\n",
        "        selected_features = selected_features_df[selected_features_df['avg_score'] == selected_features_df[\"avg_score\"].min()]['feature_names'].values[0]\n",
        "    else:\n",
        "        # Filter based on n ('n' corresponds to a specific number of features or other criterion)\n",
        "        selected_features = selected_features_df[selected_features_df['n'] == n]['feature_names'].values[0]\n",
        "\n",
        "    # Evaluate the string representation of the feature names and convert it to a list\n",
        "    selected_features = eval(selected_features)\n",
        "    selected_features = list(selected_features)\n",
        "\n",
        "    return selected_features"
      ],
      "metadata": {
        "id": "XFUyxDsKyAqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, file_path):\n",
        "  joblib.dump(model, file_path)"
      ],
      "metadata": {
        "id": "leffTITF1jtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_performance_comparison(data_path, results_root, model_eval_path):\n",
        "    # Load training data\n",
        "    X_train, y_train, X_test, y_test = load_data(path=DATA_PATH)\n",
        "\n",
        "    # Initialize cross-validation\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "    # Evaluate Baseline Model\n",
        "    baseline_model = MultiOutputRegressor(DummyRegressor(strategy=\"mean\"))\n",
        "    baseline_cv_score, baseline_test_score = evaluate_model(baseline_model, 'baseline', X_train, y_train, X_test, y_test, cv=skf)\n",
        "    # Save model\n",
        "    save_model(baseline_model, f'{model_eval_path}/baseline.pkl')\n",
        "\n",
        "    # Initialize lists to collect scores from all models, including the baseline\n",
        "    all_cv_scores = [baseline_cv_score.assign(Model='baseline', Hyperparameter_Tuning=False, Feature_Selection=False)]\n",
        "    all_test_scores = [baseline_test_score.assign(Model='baseline', Hyperparameter_Tuning=False, Feature_Selection=False)]\n",
        "\n",
        "    for model_name in ['multitask_elastic_net', 'random_forest', 'neural_network']:\n",
        "      print(f'Model: {model_name}')\n",
        "      for hyperparameter_tuning, feature_selection in [(False, False), (True, False), (True, True)]:\n",
        "          if model_name == 'neural_network' and feature_selection == True:\n",
        "            pass\n",
        "          else:\n",
        "            print(f'Hyperparameter Tuning: {hyperparameter_tuning}, Feature Selection: {feature_selection}')\n",
        "\n",
        "            # Load previously tuned hyperparameters or set default parameters\n",
        "            params = load_hyperparameters(model_name=model_name, results_root=RESULTS_ROOT, hyperparameter_tuning=hyperparameter_tuning)\n",
        "            print(f\"\\nLoading hyperparameters: {params}\")\n",
        "\n",
        "            # Initialize model\n",
        "            model = init_model(params=params, model_name=model_name, num_features=X_train.shape[1], num_targets=y_train.shape[1])\n",
        "\n",
        "            if feature_selection:\n",
        "              selected_features = load_selected_features(path=f\"{results_root}/{model_name}/hyperparameter_tuning=True-feature_selection=True\", n=None)\n",
        "\n",
        "            # Initialize cross-validation\n",
        "            skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "            # Evaluate model\n",
        "            cv_score, test_score = evaluate_model(model, model_name, X_train, y_train, X_test, y_test, cv=skf)\n",
        "\n",
        "            # Add model details to cv_score and test_score using assign\n",
        "            cv_score = cv_score.assign(\n",
        "                Model=model_name,\n",
        "                Hyperparameter_Tuning=hyperparameter_tuning,\n",
        "                Feature_Selection=feature_selection\n",
        "            )\n",
        "            test_score = test_score.assign(\n",
        "                Model=model_name,\n",
        "                Hyperparameter_Tuning=hyperparameter_tuning,\n",
        "                Feature_Selection=feature_selection\n",
        "            )\n",
        "\n",
        "            # Append to lists\n",
        "            all_cv_scores.append(cv_score)\n",
        "            all_test_scores.append(test_score)\n",
        "\n",
        "            # Save model\n",
        "            save_model(model, f'{model_eval_path}/{model_name}_{hyperparameter_tuning}_{feature_selection}.pkl')\n",
        "\n",
        "    # Concatenate all scores into final DataFrames\n",
        "    final_cv_scores = pd.concat(all_cv_scores, ignore_index=True)\n",
        "    final_test_scores = pd.concat(all_test_scores, ignore_index=True)\n",
        "\n",
        "    # Print final scores\n",
        "    print(\"Cross-Validation Scores for All Models:\")\n",
        "    print(final_cv_scores.sort_values(by='Score'))\n",
        "    print(\"\\nTest Scores for All Models:\")\n",
        "    print(final_test_scores.sort_values(by='Score'))\n",
        "\n",
        "    # Save scores\n",
        "    final_cv_scores.to_csv(f'{model_eval_path}/cv_scores.csv', index=False)\n",
        "    final_test_scores.to_csv(f'{model_eval_path}/test_scores.csv', index=False)\n",
        "\n",
        "    return final_cv_scores, final_test_scores\n",
        "\n",
        "final_cv_scores, final_test_scores = model_performance_comparison(DATA_PATH, RESULTS_ROOT, MODEL_EVAL_PATH)"
      ],
      "metadata": {
        "id": "FKcxOmhztW7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_cv_scores.to_latex()"
      ],
      "metadata": {
        "id": "bI0vYPbwGbNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_test_scores.to_latex()"
      ],
      "metadata": {
        "id": "BQh5tLGtGj7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}